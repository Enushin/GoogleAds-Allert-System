---
title: "監視指標とアラート要件"
status: "Draft"
version: "0.1.0"
language: "ja"
---

# 監視指標とアラート要件

本ドキュメントでは、Google Ads予算アラートシステムの主要メトリクス、可視化ダッシュボードの要件、およびアラート発砲条件を整理する。

> **MVP方針**: レポーティングはローンチ後の拡張対象とし、初期リリースでは以下の暫定対応のみを実施する。詳細なダッシュボード構築や自動指標集計は安定運用を確認してから着手する。

## MVP段階の暫定対応

1. ログレベル INFO 以上をSlack通知処理とGoogle Ads API呼び出しの成功/失敗で必ず出力し、Cloud Loggingなどで手動監視できる状態を維持する。
2. `python -m google_ads_alert run --dry-run` の結果を日次で保存し、予測値と実績費用を比較する軽量なレビューメモを運用チーム内で共有する。
3. 重大障害（通知送信不可、API失敗連続）を検知した場合は、[障害対応ランブック](incident_response_runbook.md) に従い手動エスカレーションする。

> **ローンチ判定までに必要なのは上記3項目のみ**。ダッシュボードや自動集計が未整備でもローンチを止めない。

### 日次ヘルスチェック・テンプレート（手動運用）
- **08:30 JST — ログ確認と記録**
  - 前回ジョブのログを確認し、Slack通知の成功/失敗件数をスプレッドシートへ転記する。
- **12:30 JST — 予測ドリフト確認**
  - `python -m google_ads_alert run --dry-run` を実行し、Slackペイロードの着地予測値が現状費用と乖離していないか簡易比較する。
- **18:30 JST — 障害スキャン**
  - 重大障害ログの有無を `grep "ERROR" logs/*.log` でチェックし、異常があれば `#mvp-launch` に共有する。

> **Note**: 上記ヘルスチェックはローンチ後1週間を目安に実施し、安定稼働が確認できたら週次レビューへ移行する。

---

以下のセクションはMVP後の監視拡張用バックログとして保持する。

## MVPで実施しない項目（明示的な保留リスト）

- BigQuery/Looker Studio等を利用したダッシュボード構築
- PrometheusやOpenTelemetryによるメトリクスエクスポート
- PagerDutyなど外部オンコールツールとの自動連携
- 予測精度の自動補正ロジックのチューニングおよび追跡

## 1. 監視対象メトリクス

| カテゴリ | 指標 | 計測方法 | アラート閾値（初期案） |
| --- | --- | --- | --- |
| Google Ads API | API呼び出し成功率 | 呼び出し成功件数 / 総リクエスト数。APSchedulerジョブ単位でログに成功/失敗イベントを記録し、ログベースメトリクスを生成する。 | 成功率 < 95%（5分移動平均）でWarning、< 90% でCritical |
| 通知配送 | Slack送信成功率 | Slack送信結果をINFO/ERRORログで記録し、失敗件数をカウント。Webhook応答コード別の集計を保持する。 | 成功率 < 98%（15分移動平均）でWarning、< 95% でCritical |
| 予測品質 | 当日着地予測誤差 (MAPE) | 実績費用と予測値を夜間バッチで比較し、日次でMAPEを算出。履歴は時系列DBに保存する。 | MAPE > 20% が3日連続でWarning、> 30% が2日連続でCritical |
| スケジューラ | ジョブ実行遅延 | ジョブ開始時刻と予定時刻の差分をログへ出力し、遅延秒数のヒストグラムを作成。 | 遅延 > 120秒が連続3回でWarning、> 300秒でCritical |
| システム健全性 | 予算計算処理時間 | 1ジョブ内での全体処理時間（API取得〜通知完了）を計測し、秒単位でログ出力。 | 処理時間 > 60秒が連続5回でWarning、> 120秒でCritical |

## 2. データ収集と計測アプローチ

1. **ログベースメトリクス**: 既存のロギングポリシーに従い、各イベントでJSON形式の追加メタデータ（`status`, `duration_ms`, `error_code` など）を構造化ログとして出力する。ログ集約基盤（例: Cloud Logging, Datadog Logs）でクエリを作成し、成功率・遅延を集計する。
2. **アプリ内メトリクス**: 将来的に `prometheus_client` などを導入し、`Counter` / `Histogram` をエクスポートする。初期段階ではログベースで十分だが、スケールに応じてHTTPエンドポイントを提供する。
3. **バッチ計測**: 予測誤差のように即時算出できない指標は、日次ジョブでCSV/BigQueryへ追記し、BIツール（Looker Studio, Data Portal等）で参照する。

## 3. ダッシュボード要件

- **リアルタイム可視化**: 成功率や処理時間を5〜15分粒度で表示。サービス全体のステータス（Normal / Warning / Critical）をトップカードとして配置する。
- **詳細ドリルダウン**: APIエラーのステータスコード別内訳、Slack送信失敗のレスポンスコード、遅延発生時のジョブIDなどをテーブル表示する。
- **履歴分析**: 日次予測誤差の推移、ジョブ遅延の分布などを週次/月次で確認できるラインチャートを用意する。
- **SLOトラッキング**: API成功率 99%、通知成功率 99.5%、予測誤差 MAPE 15% 以下を目標値として表示し、達成状況を色分けする。

## 4. アラート設計

- **通知チャネル**: 初期はSlack OpsチャンネルにPagerDuty連携前の通知を送信する。閾値のCritical超過が継続する場合のみPagerDutyにエスカレーションする。
- **エスカレーションフロー**: Warning通知は当番エンジニアが確認し、30分以内に対応開始する。Critical通知は10分以内に復旧アクション（再試行、スケジューラ再起動、通知リトライ）を実施する。
- **メンテナンスモード**: 計画停止時はアラートをサイレンスするためのスケジュール機能（例: PagerDuty Maintenance Window、Grafana AlertingのMute Timing）を利用する。

## 5. 実装ステップ（初期フェーズ）

1. CLIロギングにJSON追加情報を出力するフラグを導入し、成功/失敗イベントで構造化ログを記録する。
2. ログ集約基盤でクエリを作成し、成功率・処理時間のログベースメトリクスを定義する。
3. Slack通知成功率については `response_status` をログ化し、エラー時にWebhook URL末尾をマスクしたIDを併記する。
4. Looker Studioなどで日次予測誤差のテーブルとトレンドチャートを作成し、予測精度のレビュー会で使用する。
5. 閾値に基づくアラートルールをGrafana AlertingまたはCloud Monitoringに設定し、Slack通知テンプレートを整備する。

## 6. 今後の検討事項

- PrometheusエクスポートやOpenTelemetryメトリクスの導入によるリアルタイム監視強化。
- 予測誤差の自動補正ロジック（例: Kalman Filter、Seasonality補正）の導入と、その監視指標への組み込み。
- PagerDuty連携時の本番運用手順書作成および担当者ローテーション管理。
